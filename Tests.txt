Test 1:
    x = tf.keras.layers.Conv1D(64, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3)(x)
    x = tf.keras.layers.Conv1D(128, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3)(x)
    x = tf.keras.layers.Conv1D(512, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3)(x)
    x = lru.LRU_Block(N=256, H=256, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=256, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-10, 10)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=42000, t_mul=1, m_mul=1, alpha=0.005))
Batch size 16
Params 5.499.000
Max mean accuracy 91.8
Result: Training stopped after 2000 epochs since the loss only reduced very slowly

Test 2:
    x = tf.keras.layers.Conv1D(64, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3)(x)
    x = tf.keras.layers.Conv1D(128, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3)(x)
    x = tf.keras.layers.Conv1D(512, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3)(x)
    x = lru.LRU_Block(N=512, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = layers.ReverseLayer(axis=1)(x)
    x = lru.LRU_Block(N=512, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = layers.ReverseLayer(axis=1)(x)
    x = lru.LRU_Block(N=512, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-10, 10)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=42000, t_mul=1, m_mul=1, alpha=0.005))
Batch size 32
Params 7.599.000
Max mean accuracy 91.5
Result: Training stopped after 712 epochs since the loss only reduced very slowly

Test 4:
    x = tf.keras.layers.Conv1D(64, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3)(x)
    x = tf.keras.layers.Conv1D(128, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3)(x)
    x = tf.keras.layers.Conv1D(512, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3)(x)
    x = lru.LRU_Block(N=256, H=256, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=256, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-100, 100)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=42000, t_mul=1, m_mul=1, alpha=0.0025))
Batch size 16
Params 5.499.000
Max mean accuracy 92.0
Result: Training stopped after 2926 epochs since the loss only reduced very slowly

Test 5:
    x = tf.keras.layers.Conv1D(64, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3)(x)
    x = tf.keras.layers.Conv1D(128, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3)(x)
    x = tf.keras.layers.Conv1D(256, kernel_size=19, strides=1, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3)(x)
    x = lru.LRU_Block(N=16, H=256, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=16, H=256, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=16, H=256, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=16, H=256, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=16, H=256, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-10, 10)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=6e-4, first_decay_steps=42000, t_mul=1, m_mul=1, alpha=0.0025))
Params 1.453.000
Max mean accuracy 90.1
Result: Training stopped after 771 epochs since the loss did not reduce significantly during the last few iterations

Test 6:
    x = tf.keras.layers.Conv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(512, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=256, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = layers.LinearCRFEncoder(512, 4, 5, bias=False, blank_score=2.0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=42000, t_mul=1, m_mul=1, alpha=0.00025))
Batch size 16
Params 6.386.000
Max mean accuracy 92.7
Result: Training stopped after 2851 epochs since the loss only kept declining very slow

Test 7:
    x = tf.keras.layers.Conv1D(128, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=16, H=16, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=16, H=16, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=16, H=16, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=42000, t_mul=1, m_mul=1, alpha=0.00025))
Batch size 16
Params 15.600
Max mean accuracy 77.9
Result: Training stopped after 3765 epochs since the loss only kept declining very slow

Test 9:
    x = tf.keras.layers.Conv1D(1028, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=16, H=16, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=16, H=16, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=16, H=16, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=42000, t_mul=1, m_mul=1, alpha=0.00025))
Batch size 16
Prams 85.800
Max mean accuracy 76.3
Result: Training stopped after 910 epochs since the loss only kept declining very slow

Test 10:
    x = tf.keras.layers.SeparableConv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = tf.keras.layers.SeparableConv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = tf.keras.layers.SeparableConv1D(128, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=42000, t_mul=1, m_mul=1, alpha=0.00025))
Batch size 16
Params 2.082.000
Max mean accuracy 91.0
Result: Training stopped after 4503 epochs since the loss only kept declining very slow

Test 11:
    x = tf.keras.layers.Conv1D(128, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=64, H=64, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=64, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=64, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=42000, t_mul=1, m_mul=1, alpha=0.00025))
Batch size 16
Params 118.000
Max mean accuracy 85.1
Result: Training stopped after 2199 epochs since the loss only kept declining very slow

Test 14:
    x = tf.keras.layers.Conv1D(128, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=128, H=128, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=128, H=128, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=128, H=128, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=42000, t_mul=1, m_mul=1, alpha=0.00025))
Batch size 16
Params 398.000
Max mean accuracy 88.2
Result: Training stopped after 2029 epochs since the loss only kept declining very slow

Test 15:
    x = tf.keras.layers.Conv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=128, H=128, bidirectional=True, max_phase=2*pi/1, dropout=0)(x)
    x = tf.keras.layers.Conv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(1024, kernel_size=19, strides=1, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=96000, t_mul=1, m_mul=1, alpha=0.000000005))
Batch size 16
Params 3.039.00
Max mean accuracy 94.4
Result: Training stopped after 1983 epochs since the loss only kept declining very slow

Test 16:
    x = tf.keras.layers.Conv1D(512, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=256, H=256, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=256, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=256, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=96000, t_mul=1, m_mul=1, alpha=0.000000005))
Batch size 16
Params 1.850.000
Max mean accuracy 90.9
Result: Training stopped after 1698 epochs since the loss only kept declining very slow, however, convergence was not reached yet

Test 18:
    x = tf.keras.layers.Conv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(1024, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=124000, t_mul=1, m_mul=1, alpha=0.00000000005))
Batch size 16
Params 4.390.000
Max mean accuracy 94.5
Result: Training stopped after 5361 epochs since the loss only kept declining very slow, if at all, convergence seemed to be reached

Test 19:
    x = tf.keras.layers.Conv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(1024, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=124000, t_mul=1.2, m_mul=1, alpha=0.00000000005))
Batch size 16
Params 5.838.000
Max mean accuracy 95.4
Result: The Loss did not converge yet, but after 2970 epochs the loss went up sharply. This might be due to the number of LRUs used.

Test 20:
    x = tf.keras.layers.Conv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    residual = x
    x = tf.keras.layers.Conv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(1024, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    residual = tf.keras.layers.AveragePooling1D(pool_size=6, strides=6, padding='same')(residual)
    residual = tf.keras.layers.Dense(1024, use_bias=False)(residual)
    x = tf.keras.layers.add([residual, x])
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=124000, t_mul=1.2, m_mul=1, alpha=0.00000000005))
Batch size 16
Params 4.422.000
Max mean accuracy 95.2
Min loss 94.8
Result: Training stopped after 7929 epochs since the loss did not decrease anymore

Test 21:
    x = tf.keras.layers.Conv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    residual = x
    x = tf.keras.layers.Conv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(1024, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    residual = tf.keras.layers.AveragePooling1D(pool_size=6, strides=6, padding='same')(residual)
    residual = tf.keras.layers.Dense(1024, use_bias=False)(residual)
    x = tf.keras.layers.add([residual, x])
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.PolynomialDecay(2e-4, decay_steps=1000000, end_learning_rate=8e-6, power=1.0))
Batch size 16
Params 4.422.000
Max mean accuracy 93.7
Min loss 116.8
Result: Training stopped after 2952 epochs as the loss did not really decrease anymore

Test 23:
    x = tf.keras.layers.Conv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(1024, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, r_min=0.9, r_max=0.999, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, r_min=0.9, r_max=0.999, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, r_min=0.9, r_max=0.999, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, r_min=0.9, r_max=0.999, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, r_min=0.9, r_max=0.999, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.PolynomialDecay(2e-4, decay_steps=2500000, end_learning_rate=1e-18, power=1.0))
Batch size 16
Params 4.390.000
Max mean accuracy 94.7
Min loss 98.5
Result: The loss keeps declining steadily but extremely slowly. This is strange since the end learning rate is extremely low.
Training was stopped after 2547 epochs with the loss not on a plateau yet.

Test 26:
    x = tf.keras.layers.Conv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(32, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(1024, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=256, H=128, bidirectional=True, r_min=0, r_max=1, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=128, bidirectional=True, r_min=0, r_max=1, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=128, bidirectional=True, r_min=0, r_max=1, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=128, bidirectional=True, r_min=0, r_max=1, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=128, bidirectional=True, r_min=0, r_max=1, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.PolynomialDecay(4e-4, decay_steps=150000, end_learning_rate=8e-5, power=1.0))
Batch size 32
Params 2.710.000
Max mean accuracy 90.6
Min loss 155.8
Result: Loss did not converge yet, however the loss kept decreasing only extremely slow. Training stopped after 1585 epochs.

Test 27:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=32, H=32, bidirectional=False, max_phase=2*pi/20)(x)
    x = layers.ReverseLayer(axis=1)(x)
    x = lru.LRU_Block(N=32, H=32, bidirectional=False, max_phase=2*pi/20)(x)
    x = layers.ReverseLayer(axis=1)(x)
    x = lru.LRU_Block(N=32, H=32, bidirectional=False, max_phase=2*pi/20)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=1e-4, warmup_end=4e-4, warmup_steps=400000,
        initial_learning_rate=1e-4, decay_steps=400000, alpha=1e-8, t_mul=1.2, m_mul=1.0))
Batch size 16
Params 163.000
Max mean accuracy 81.0
Min loss 296.9
Result: The loss only kept reducing extremely slowly, even on a comparibly high loss value. Training was stopped after 341 epochs (Steps per epoch 5000).

Test 28:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=64, H=64, bidirectional=False, max_phase=2*pi/20)(x)
    x = layers.ReverseLayer(axis=1)(x)
    x = lru.LRU_Block(N=64, H=64, bidirectional=False, max_phase=2*pi/20)(x)
    x = layers.ReverseLayer(axis=1)(x)
    x = lru.LRU_Block(N=64, H=64, bidirectional=False, max_phase=2*pi/20)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=1e-4, warmup_end=4e-4, warmup_steps=400000,
        initial_learning_rate=1e-4, decay_steps=400000, alpha=1e-8, t_mul=1.2, m_mul=1.0))
Batch size 16
Params 238.000
Max mean accuracy 83.8
Min loss 259.1
Result: The loss only kept reducing extremely slowly, even on a comparibly high loss value. Training was stopped after 283 epochs (Steps per epoch 5000).

Test 29:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=256, H=160, bidirectional=False, max_phase=2*pi/20)(x)
    x = layers.ReverseLayer(axis=1)(x)
    x = lru.LRU_Block(N=256, H=160, bidirectional=False, max_phase=2*pi/20)(x)
    x = layers.ReverseLayer(axis=1)(x)
    x = lru.LRU_Block(N=256, H=160, bidirectional=False, max_phase=2*pi/20)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=164000, t_mul=1.2, m_mul=0.8, alpha=0.00000000005))
Batch size 48
Params 886.000
Max mean accuracy 87.6
Min loss 199.2
Result: Training stopped after 3680 epochs as the loss did barely decrease anymore.

Test 31:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=64, H=256, bidirectional=True, max_phase=2*pi/20)(x)
    x = lru.LRU_Block(N=64, H=256, bidirectional=True, max_phase=2*pi/20)(x)
    x = lru.LRU_Block(N=64, H=256, bidirectional=True, max_phase=2*pi/20)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=2e-3, warmup_end=6e-4, warmup_steps=100000,
        initial_learning_rate=2e-4, decay_steps=200000, alpha=1e-8, t_mul=1.2, m_mul=1.0))
Batch size 32
Params 848.000
Max mean accuracy 89.6
Result: Training stopped after 629 epochs (5000 steps per epoch) as the loss decreased extremely slowly.

Test 32:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    residual = x
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    residual = layers.DynamicPaddingLayer()(residual)
    residual = tf.keras.layers.Reshape((-1, 6, 16))(residual)
    residual = tf.keras.layers.Reshape((-1, 16 * 6))(residual)
    residual = tf.keras.layers.Dense(384, use_bias=False)(residual)
    x = tf.keras.layers.add([residual, x])
    x = lru.LRU_Block(N=64, H=256, bidirectional=True, max_phase=2*pi/40)(x)
    x = lru.LRU_Block(N=64, H=256, bidirectional=True, max_phase=2*pi/40)(x)
    x = lru.LRU_Block(N=64, H=256, bidirectional=True, max_phase=2*pi/40)(x)
    x = lru.LRU_Block(N=64, H=256, bidirectional=True, max_phase=2*pi/40)(x)
    x = lru.LRU_Block(N=64, H=256, bidirectional=True, max_phase=2*pi/40)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.Adam(learning_rate=schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=1e-4, warmup_end=1e-4, warmup_steps=72000,
        initial_learning_rate=1e-4, decay_steps=16000, alpha=0.05, t_mul=1.05, m_mul=1.0), epsilon=1e-16)
Batch size 32
Max mean accuracy 93.9
Min loss 106
Result: Training ended after 10240 epochs.

Test 33:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    residual = x
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    residual = layers.DynamicPaddingLayer()(residual)
    residual = tf.keras.layers.Reshape((-1, 6, 16))(residual)
    residual = tf.keras.layers.Reshape((-1, 16 * 6))(residual)
    residual = tf.keras.layers.Dense(384, use_bias=False)(residual)
    x = tf.keras.layers.add([residual, x])
    x = lru.LRU_Block(N=384, H=384, bidirectional=True, max_phase=2*pi/20)(x)
    x = lru.LRU_Block(N=384, H=384, bidirectional=True, max_phase=2*pi/20)(x)
    x = lru.LRU_Block(N=384, H=384, bidirectional=True, max_phase=2*pi/20)(x)
    x = lru.LRU_Block(N=384, H=384, bidirectional=True, max_phase=2*pi/20)(x)
    x = lru.LRU_Block(N=384, H=384, bidirectional=True, max_phase=2*pi/20)(x)
    optimizer = tf.keras.optimizers.Adam(learning_rate=schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=9e-5, warmup_end=1e-4, warmup_steps=800000,
        initial_learning_rate=9e-5, decay_steps=20000, alpha=0.0011, t_mul=1.05, m_mul=1.0), epsilon=1e-16)
Batch size 32
Params 6.076.000
Max mean accuracy 93.9
Min loss 100.9
Result: Training stopped after 563 epochs (10000 steps per epoch). The loss does not seem to decrease much past 100.

Test 34:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    residual = x
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    residual = layers.DynamicPaddingLayer()(residual)
    residual = tf.keras.layers.Reshape((-1, 6, 16))(residual)
    residual = tf.keras.layers.Reshape((-1, 16 * 6))(residual)
    residual = tf.keras.layers.Dense(384, use_bias=False)(residual)
    x = tf.keras.layers.add([residual, x])
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20)(x)
    x = lru.LRU_Block(N=64, H=512, bidirectional=True, max_phase=2*pi/20)(x)
    temporal_context = layers.TemporalContextLayer(128)(inputs)
    temporal_context = layers.DynamicPaddingLayer()(temporal_context)
    x = tf.keras.layers.Conv1DTranspose(filters=257, kernel_size=6, strides=6, padding='valid')(x)
    x = tf.keras.layers.add([temporal_context, x])
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/1)(x)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=1e-4, warmup_end=2e-5, warmup_steps=864000,
        initial_learning_rate=8e-5, decay_steps=16000, alpha=0.1, t_mul=1.001, m_mul=1.0), epsilon=1e-16)
Batch size 32
Params 7.428.000
Max mean accuracy 92.8
Min loss 105.1
Result: Training stopped after 1121 epochs as the loss did not decrease anymore.

Test 35:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    residual = x
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    residual = layers.DynamicPaddingLayer()(residual)
    residual = tf.keras.layers.Reshape((-1, 6, 16))(residual)
    residual = tf.keras.layers.Reshape((-1, 16 * 6))(residual)
    residual = tf.keras.layers.Dense(384, use_bias=False)(residual)
    x = tf.keras.layers.add([residual, x])
    x = lru.LRU_Block(N=48, H=768, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    x = lru.LRU_Block(N=48, H=768, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    x = lru.LRU_Block(N=48, H=768, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    x = lru.LRU_Block(N=48, H=768, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    x = lru.LRU_Block(N=48, H=768, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    optimizer = tf.keras.optimizers.Adam(learning_rate=schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=4e-4, warmup_end=2e-4, warmup_steps=1400000,
        initial_learning_rate=9e-5, decay_steps=12000, alpha=0.12, t_mul=1.01, m_mul=1.0), epsilon=1e-16)
Batch size 16
Params 7.113.000
Max mean accuracy 94.8
Min loss 98.5
Result: Training stopped after 3403 epochs as the loss barely decreased anymore.

Test 36:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True, dtype=tf.float32)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5, dtype=tf.float32)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True, dtype=tf.float32)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5, dtype=tf.float32)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True, dtype=tf.float32)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5, dtype=tf.float32)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/200, r_min=0.01, r_max=1, dropout=0.9)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/200, r_min=0.01, r_max=1, dropout=0.9)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/200, r_min=0.01, r_max=1, dropout=0.9)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/200, r_min=0.01, r_max=1, dropout=0.9)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/200, r_min=0.01, r_max=1, dropout=0.9)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-50, 50)(classes)
    scheduler = schedulers.LRReductionScheduler(
             initial_lr=2e-4,
             patience=100,
             reduce_difference=-1,
             factor=0.9,
             wait_steps=110,
             min_lr=1e-8,
             reset=True)
    optimizer = tf.keras.optimizers.Adam(learning_rate=scheduler, weight_decay=0, epsilon=1e-10)
Batch size 16
Params 17.853.000
Max mean accuracy 93.2
Min loss 127.0
Result: Training stopped after 732 epochs as it spiraled out of control after 436 epochs.

Test 37:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    residual = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.add([residual, x])
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/1000, r_min=0.05, r_max=1, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/1000, r_min=0.05, r_max=1, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/1000, r_min=0.05, r_max=1, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-50, 50)(classes)
    scheduler = schedulers.LRReductionScheduler(
             initial_lr=8e-4,
             patience=60,
             reduce_difference=0.8,
             factor=0.8,
             wait_steps=80,
             min_lr=1e-10,
             reset=True)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, clipnorm=1, weight_decay=0, epsilon=1e-10)
Batch size 16
Params 10.506.000
Max mean accuracy 93.6
Min loss 113.8
Result: Training stopped after 1775 epochs as the loss barely decreased anymore and there is much overfitting.

Test 38:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True, dtype=tf.float32)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5, dtype=tf.float32)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True, dtype=tf.float32)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5, dtype=tf.float32)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True, dtype=tf.float32)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5, dtype=tf.float32)(x)
    x = lru.LRU_Block(N=384, H=512, bidirectional=True, max_phase=2*pi/20, r_min=0.001, r_max=0.999, dropout=0.5)(x)
    x = lru.LRU_Block(N=384, H=512, bidirectional=True, max_phase=2*pi/20, r_min=0.001, r_max=0.999, dropout=0.5)(x)
    x = lru.LRU_Block(N=384, H=512, bidirectional=True, max_phase=2*pi/20, r_min=0.001, r_max=0.999, dropout=0.5)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=1800000, t_mul=1.1, m_mul=1, alpha=0.000005)
    optimizer = tf.keras.optimizers.Adam(learning_rate=scheduler, weight_decay=0.01)
Batch size 16
Params 5.051.000
Max mean accuracy 93.3
Min loss 122
Result: Training stopped after 6563 epochs as the scheduler resetted for the third time and the loss decrease was very slow.

Test 39:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True, dtype=tf.float32)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5, dtype=tf.float32)(x)
    residual2 = x
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True, dtype=tf.float32)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5, dtype=tf.float32)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True, dtype=tf.float32)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5, dtype=tf.float32)(x)
    residual1 = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True, dtype=tf.float32)(inputs)
    residual1 = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5, dtype=tf.float32)(residual1)
    residual2 = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True, dtype=tf.float32)(residual2)
    residual2 = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5, dtype=tf.float32)(residual2)
    x = tf.keras.layers.Concatenate(axis=-1)([residual1, residual2, x])
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/10, dropout=0.7)(x)
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/10, dropout=0.7)(x)
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/10, dropout=0.7)(x)
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/10, dropout=0.7)(x)
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/10, dropout=0.7)(x)
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/10, dropout=0.7)(x)
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/10, dropout=0.7)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=1e-4, first_decay_steps=2200000, t_mul=1.1, m_mul=1, alpha=0.000005)
    optimizer = tf.keras.optimizers.Adam(learning_rate=scheduler, weight_decay=0.01)
Batch size 16
Params 4.794.000
Max mean accuracy 95.8
Min loss 83.3
Result: Training stopped after 7314 epochs as the start of a new cycle. The loss might further decrease but slowly.

Test 40:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    resx = x
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    resx = layers.DynamicPaddingLayer()(resx)
    resx = tf.keras.layers.Reshape((-1, 6, 16))(resx)
    resx = tf.keras.layers.Reshape((-1, 16 * 6))(resx)
    resx = tf.keras.layers.Dense(384, use_bias=False)(resx)
    x = tf.keras.layers.add([resx, x])
    x = lru.LRU_Block(N=96, H=768, bidirectional=True, max_phase=2*pi/5)(x)
    x = lru.LRU_Block(N=96, H=768, bidirectional=True, max_phase=2*pi/5)(x)
    x = lru.LRU_Block(N=96, H=768, bidirectional=True, max_phase=2*pi/5)(x)
    x = lru.LRU_Block(N=96, H=768, bidirectional=True, max_phase=2*pi/5)(x)
    x = lru.LRU_Block(N=96, H=768, bidirectional=True, max_phase=2*pi/5)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=1e-4, first_decay_steps=164000, t_mul=1.2, m_mul=1, alpha=0.000000000000001))
Batch size 32
Params 8.146.000
Max mean accuracy 95.1
Min loss 90.2
Result: Training stopped after 5680 epochs.

Test 41:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.Adam(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=2000000, t_mul=1.1, m_mul=1, alpha=0.001))
Batch size 16
Params 17.853.000
Max mean accuracy 95.8
Min loss 74.3
Result: Training stopped after 2565 epochs.

Test 42:

    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.Adam(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=2800000, t_mul=1.1, m_mul=1, alpha=0.0005), weight_decay=0.01)
Batch size 16
Params 17.853.000
Max mean accuracy 96.0
Min loss 67.3
Result: Training stopped after 2730 epochs.

Test 43:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.9)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.Adam(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=2800000, t_mul=1.1, m_mul=1, alpha=0.0005), weight_decay=0.05)
Batch size 16
Params 17.853.000
Max mean accuracy 93.9
Min loss 112.5
Result: Training stopped after 3294 epochs.

Test 44:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.7)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.7)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.7)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.7)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi/20, dropout=0.7)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=False, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    optimizer = tf.keras.optimizers.Adam(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(initial_learning_rate=2e-4, first_decay_steps=2800000, t_mul=1.1, m_mul=1, alpha=0.0005), weight_decay=0.01)
Batch size 16
Params 17.853.000
Max mean accuracy 95.8
Min loss 72.7
Result: Training ended after 5392 epochs.

Test 45:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    residual = x
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    residual = tf.keras.layers.Conv1D(128, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(residual)
    residual = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(residual)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = tf.keras.layers.Concatenate(axis=-1)([residual, x])
    x = lru.LRU_Block(N=24, H=512, bidirectional=True, max_phase=2*pi/1, r_min=0.6, dropout=0.7)(x)
    x = lru.LRU_Block(N=24, H=512, bidirectional=True, max_phase=2*pi/1, r_min=0.6, dropout=0.7)(x)
    x = lru.LRU_Block(N=24, H=512, bidirectional=True, max_phase=2*pi/20, r_min=0.9, dropout=0.7)(x)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=1e-7, warmup_end=1e-4, warmup_steps=100000,
        initial_learning_rate=2e-4, decay_steps=3200000, alpha=0.48, t_mul=1.5, m_mul=0.96)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.005)
Batch size 16
Params 1.965.000
Max mean accuracy 89
Min loss 173
Result: Training stopped after 3468. Loss was going down extremely slowly and might have declined a bit further.

Test 46:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=256, H=128, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=128, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=128, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=128, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=128, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=128, bidirectional=True, max_phase=2*pi/20, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=8e-5, warmup_end=2e-4, warmup_steps=100000,
        initial_learning_rate=2e-4, decay_steps=5200000, alpha=0.25, t_mul=1.5, m_mul=0.96)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.01)
Batch size 32
Min loss 419
Result: This leads to the Lambda grouping at a supposedly suboptimal position (0.25,0.25). This might have had wrong LRU code.

Test 47:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = lru.LRU_Block(N=256, H=128, bidirectional=True, max_phase=2*pi/3, r_min=0.7, r_max=0.9, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=128, bidirectional=True, max_phase=2*pi/3, r_min=0.6, r_max=0.9, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=128, bidirectional=True, max_phase=2*pi/3, r_min=0.6, r_max=0.9, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=128, bidirectional=True, max_phase=2*pi/3, r_min=0.6, r_max=0.9, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=128, bidirectional=True, max_phase=2*pi/3, r_min=0.6, r_max=0.9, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=128, bidirectional=True, max_phase=2*pi/3, r_min=0.6, r_max=0.9, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=8e-5, warmup_end=2e-4, warmup_steps=100000,
        initial_learning_rate=2e-4, decay_steps=5200000, alpha=0.25, t_mul=1.5, m_mul=0.96)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.01)
Params 1.770.000
Max mean accuracy 91.6
Min loss 145.0
Result: Training stopped after 5092 epochs. The LRU parameters where starting to group at specific points. The loss did not decrease anymore for about 1000 epochs.

Test 48:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = lru.LRU_Block(N=64, H=1280, bidirectional=True, max_phase=2*pi/50, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=1280, bidirectional=True, max_phase=2*pi/50, dropout=0)(x)
    x = lru.LRU_Block(N=64, H=1280, bidirectional=True, max_phase=2*pi/50, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=8e-5, warmup_end=2e-4, warmup_steps=100000,
        initial_learning_rate=2e-4, decay_steps=5200000, alpha=0.25, t_mul=1.5, m_mul=0.96)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.01)
Params 11.222.000
Max mean accuracy 93.1
Min loss 120.2
Result: Training stopped after 4214 epochs as the loss only kept declining very slowly.

Test 49:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = lru.LRU_Block(N=320, H=1024, bidirectional=True, max_phase=2*pi/3, r_min=0.55, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=320, H=1024, bidirectional=True, max_phase=2*pi/3, r_min=0.55, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=320, H=1024, bidirectional=True, max_phase=2*pi/3, r_min=0.55, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=320, H=1024, bidirectional=True, max_phase=2*pi/3, r_min=0.55, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=320, H=1024, bidirectional=True, max_phase=2*pi/3, r_min=0.55, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=320, H=1024, bidirectional=True, max_phase=2*pi/3, r_min=0.55, r_max=0.85, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=8e-5, warmup_end=1e-4, warmup_steps=100000,
        initial_learning_rate=1e-4, decay_steps=6200000, alpha=0.45, t_mul=1.5, m_mul=0.96)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.001)
Params 23.727.000
Max mean accuracy 96.0
Min loss 66.8
Result: nan values after 2620 epochs. Overfitting is visible.

Test 50:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi, r_min=0.01, r_max=0.99, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi, r_min=0.01, r_max=0.99, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi, r_min=0.01, r_max=0.99, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi, r_min=0.01, r_max=0.99, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi, r_min=0.01, r_max=0.99, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi, r_min=0.01, r_max=0.99, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi, r_min=0.01, r_max=0.99, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1024, bidirectional=True, max_phase=2*pi, r_min=0.01, r_max=0.99, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=6e-5, warmup_end=7e-5, warmup_steps=100000,
        initial_learning_rate=7e-5, decay_steps=6200000, alpha=0.045, t_mul=1.5, m_mul=0.96)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.005)
Params 28.884.000
Max mean accuracy 95.3
Min loss 82.1
Result: nan values after 690 epochs. It does not seem like that the lambda values for every layer moved to
a point where they are supposed to be.

Test 51:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi/3, r_min=0.8, r_max=0.95, dropout=0)(x)
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi/3, r_min=0.6, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi/3, r_min=0.6, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi/3, r_min=0.6, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi/3, r_min=0.6, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi/3, r_min=0.6, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi/3, r_min=0.6, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi/5, r_min=0.8, r_max=0.95, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=8e-5, warmup_end=2e-4, warmup_steps=100000,
        initial_learning_rate=2e-4, decay_steps=6200000, alpha=0.45, t_mul=1.5, m_mul=0.96)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.001)
Params 6.654.000
Max mean accuracy 95.7
Min loss 79.7
Result: Time limit reached at 4252 epochs. No overfitting can be seen. Lambdas seem to have converged.

Test 52:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi, r_min=0.8, r_max=0.95, dropout=0)(x)
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi, r_min=0.65, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi, r_min=0.65, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi, r_min=0.65, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi, r_min=0.65, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi, r_min=0.65, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi, r_min=0.65, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi, r_min=0.65, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi, r_min=0.65, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=96, H=512, bidirectional=True, max_phase=2*pi, r_min=0.65, r_max=0.85, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=8e-5, warmup_end=9e-5, warmup_steps=100000,
        initial_learning_rate=9e-5, decay_steps=6200000, alpha=0.45, t_mul=1.5, m_mul=0.96)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.001)
Params 8.299.000
Max mean accuracy 94
Min loss 100
Result: Training stopped after 995 epochs as the loss was nan.

Test 53:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = lru.LRU_Block(N=256, H=1536, bidirectional=True, max_phase=2*pi/3, r_min=0.8, r_max=0.95, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1536, bidirectional=True, max_phase=2*pi/3, r_min=0.6, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1536, bidirectional=True, max_phase=2*pi/3, r_min=0.6, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1536, bidirectional=True, max_phase=2*pi/3, r_min=0.6, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1536, bidirectional=True, max_phase=2*pi/3, r_min=0.6, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1536, bidirectional=True, max_phase=2*pi/3, r_min=0.6, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1536, bidirectional=True, max_phase=2*pi/3, r_min=0.6, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=256, H=1536, bidirectional=True, max_phase=2*pi/5, r_min=0.8, r_max=0.95, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=8e-5, warmup_end=2e-4, warmup_steps=100000,
        initial_learning_rate=8e-5, decay_steps=6200000, alpha=0.45, t_mul=1.5, m_mul=0.96)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.005)
Params 55.648.000
Max mean accuracy 95.8
Min loss 70.9
Result: Training stopped after 1229 epochs due to time limit. Visible overfitting.

Test 54:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = lru.LRU_Block(N=768, H=768, bidirectional=True, max_phase=2*pi, r_min=0.55, r_max=0.95, dropout=0)(x)
    x = lru.LRU_Block(N=768, H=768, bidirectional=True, max_phase=2*pi, r_min=0.55, r_max=0.95, dropout=0)(x)
    x = lru.LRU_Block(N=768, H=768, bidirectional=True, max_phase=2*pi, r_min=0.55, r_max=0.95, dropout=0)(x)
    x = lru.LRU_Block(N=768, H=768, bidirectional=True, max_phase=2*pi, r_min=0.55, r_max=0.95, dropout=0)(x)
    x = lru.LRU_Block(N=768, H=768, bidirectional=True, max_phase=2*pi, r_min=0.55, r_max=0.95, dropout=0)(x)
    x = lru.LRU_Block(N=768, H=768, bidirectional=True, max_phase=2*pi, r_min=0.55, r_max=0.95, dropout=0)(x)
    x = lru.LRU_Block(N=768, H=768, bidirectional=True, max_phase=2*pi, r_min=0.55, r_max=0.95, dropout=0)(x)
    x = lru.LRU_Block(N=768, H=768, bidirectional=True, max_phase=2*pi, r_min=0.55, r_max=0.95, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=6e-5, warmup_end=8e-5, warmup_steps=100000,
        initial_learning_rate=8e-5, decay_steps=6200000, alpha=0.45, t_mul=1.5, m_mul=0.96)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.002)
Params 36.753.000
Max mean accuracy 95.4
Min loss 80.1
Result: Training stopped after 1077 epochs, due to time limit. Slight overfitting visible. Loss did not converge yet. Lambdas for middle layers moved little. Lambdas for first and last layers moved to what seems to be their optimal positions.

Test 55:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = lru.LRU_Block(N=128, H=2048, bidirectional=True, max_phase=2*pi/4, r_min=0.92, r_max=0.98, dropout=0.9)(x)
    x = lru.LRU_Block(N=128, H=2048, bidirectional=True, max_phase=2*pi/3, r_min=0.66, r_max=0.74, dropout=0.9)(x)
    x = lru.LRU_Block(N=128, H=2048, bidirectional=True, max_phase=2*pi/3, r_min=0.66, r_max=0.74, dropout=0.9)(x)
    x = lru.LRU_Block(N=128, H=2048, bidirectional=True, max_phase=2*pi/3, r_min=0.66, r_max=0.74, dropout=0.9)(x)
    x = lru.LRU_Block(N=128, H=2048, bidirectional=True, max_phase=2*pi/3, r_min=0.66, r_max=0.74, dropout=0.9)(x)
    x = lru.LRU_Block(N=128, H=2048, bidirectional=True, max_phase=2*pi/3, r_min=0.66, r_max=0.74, dropout=0.9)(x)
    x = lru.LRU_Block(N=128, H=2048, bidirectional=True, max_phase=2*pi/3, r_min=0.7, r_max=0.85, dropout=0.9)(x)
    x = lru.LRU_Block(N=128, H=2048, bidirectional=True, max_phase=2*pi/5, r_min=0.8, r_max=0.95, dropout=0.9)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=7e-5, warmup_end=8e-5, warmup_steps=100000,
        initial_learning_rate=8e-5, decay_steps=4200000, alpha=0.45, t_mul=1.5, m_mul=0.96)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.01)
Params 79.065.000
Max mean accuracy 95.9
Min loss 79.2
Result: Training ended after 1901 epochs due to time limit. Very slight overfitting visible. Loss had not converged fully yet. Lambdas for all layers seem to have moved to their optimal positions.

Test 56:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = lru.LRU_Block(N=1024, H=1024, bidirectional=True, max_phase=2*pi/4, r_min=0.92, r_max=0.98, dropout=0.4)(x)
    x = lru.LRU_Block(N=1024, H=1024, bidirectional=True, max_phase=2*pi/3, r_min=0.66, r_max=0.74, dropout=0.4)(x)
    x = lru.LRU_Block(N=1024, H=1024, bidirectional=True, max_phase=2*pi/3, r_min=0.66, r_max=0.74, dropout=0.4)(x)
    x = lru.LRU_Block(N=1024, H=1024, bidirectional=True, max_phase=2*pi/3, r_min=0.66, r_max=0.74, dropout=0.4)(x)
    x = lru.LRU_Block(N=1024, H=1024, bidirectional=True, max_phase=2*pi/3, r_min=0.66, r_max=0.74, dropout=0.4)(x)
    x = lru.LRU_Block(N=1024, H=1024, bidirectional=True, max_phase=2*pi/3, r_min=0.66, r_max=0.74, dropout=0.4)(x)
    x = lru.LRU_Block(N=1024, H=1024, bidirectional=True, max_phase=2*pi/3, r_min=0.7, r_max=0.85, dropout=0.4)(x)
    x = lru.LRU_Block(N=1024, H=1024, bidirectional=True, max_phase=2*pi/5, r_min=0.8, r_max=0.95, dropout=0.4)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=7e-5, warmup_end=8e-5, warmup_steps=100000,
        initial_learning_rate=8e-5, decay_steps=4200000, alpha=0.45, t_mul=1.5, m_mul=0.96)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.01)
Params 64.692.000
Max mean accuracy 95.5
Min loss 90.5
Result: Training ended after 1194 epochs due to time limit. No visible overfitting yet. Loss did not converge yet. It is unclear whether the Lambdas in all layers moved to their optimal positions, most likely not yet.

Test 57:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = lru.LRU_Block(N=2048, H=1024, bidirectional=True, max_phase=2*pi*(2/3), r_min=0.85, r_max=0.95, dropout=0)(x)
    x = lru.LRU_Block(N=2048, H=1024, bidirectional=True, max_phase=2*pi, r_min=0.02, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=2048, H=1024, bidirectional=True, max_phase=2*pi, r_min=0.68, r_max=0.74, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=6e-5, warmup_end=2e-4, warmup_steps=200000,
        initial_learning_rate=2e-4, decay_steps=4200000, alpha=0.0045, t_mul=1.5, m_mul=0.96)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.002)
Params 38.963.000
Max mean accuracy 92.1
Min loss 116.9
Result: Training ended after 1266 epochs due to time limit. Loss did not converge yet but there is extreme overfitting. The Lambdas seem to have moved to their positions.

Test 58:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/2, r_min=0.90, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/3, r_min=0.85, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/3, r_min=0.52, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/3, r_min=0.52, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/3, r_min=0.52, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/3, r_min=0.52, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/3, r_min=0.52, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/3, r_min=0.52, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/3, r_min=0.52, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/3, r_min=0.52, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/3, r_min=0.65, r_max=0.80, dropout=0)(x)
    x = lru.LRU_Block(N=128, H=384, bidirectional=True, max_phase=2*pi/4, r_min=0.68, r_max=0.75, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=8e-5, warmup_end=1e-4, warmup_steps=100000,
        initial_learning_rate=1e-4, decay_steps=3200000, alpha=0.6, t_mul=0.06, m_mul=0.5)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.01)
Params 7.233.000
Max mean accuracy 94.2
Min loss 88.0
Result: Training stopped after 84 full epochs due to time limit. Training seems to have nearly converged. There is slight overfitting. Lambdas seem to have mostly converged.

Test 59:
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = lru.LRU_Block(N=512, H=1536, bidirectional=True, max_phase=2*pi/2, r_min=0.90, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=1536, bidirectional=True, max_phase=2*pi/3, r_min=0.85, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=1536, bidirectional=True, max_phase=2*pi/3, r_min=0.52, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=1536, bidirectional=True, max_phase=2*pi/3, r_min=0.60, r_max=0.85, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=1536, bidirectional=True, max_phase=2*pi/4, r_min=0.68, r_max=0.75, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=8e-5, warmup_end=1e-4, warmup_steps=100000,
        initial_learning_rate=1e-4, decay_steps=3200000, alpha=0.6, t_mul=0.06, m_mul=0.5)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.01)
Params 45.006.000
Max mean accuracy 94.4
Min loss 71.9
Result: Training stopped after 43 epochs due to time limit. Heavy overfitting and validation loss did not decrease anymore. Accuracy was still slightly increasing. Lambdas seem to have converged to the expected positions.

Test 60:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = lru.LRU_Block(N=384, H=384, bidirectional=True, max_phase=2*pi/2, r_min=0.90, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=384, H=384, bidirectional=True, max_phase=2*pi/3, r_min=0.85, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=384, H=384, bidirectional=True, max_phase=2*pi/3, r_min=0.52, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=384, H=384, bidirectional=True, max_phase=2*pi/3, r_min=0.52, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=384, H=384, bidirectional=True, max_phase=2*pi/3, r_min=0.52, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=384, H=384, bidirectional=True, max_phase=2*pi/3, r_min=0.52, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=384, H=384, bidirectional=True, max_phase=2*pi/3, r_min=0.65, r_max=0.80, dropout=0)(x)
    x = lru.LRU_Block(N=384, H=384, bidirectional=True, max_phase=2*pi/4, r_min=0.68, r_max=0.75, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=8e-5, warmup_end=1e-4, warmup_steps=100000,
        initial_learning_rate=1e-4, decay_steps=3200000, alpha=0.6, t_mul=0.06, m_mul=0.5)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.01)
Params 9.590.000
Max mean accuracy 93.6
Min loss 96.6
Result: Training stopped after 71 full epochs due to time limit. Loss was still decreasing, but slowly. Slight overfitting visible. Lambdas in most layers converged, but maybe not all.

Test 61:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(64, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(64, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(384, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = lru.LRU_Block(N=1024, H=144, bidirectional=True, max_phase=2*pi/10, r_min=0.05, r_max=0.95, use_skip_connection=True, dropout=0)(x)
    x = lru.LRU_Block(N=1024, H=144, bidirectional=True, max_phase=2*pi/10, r_min=0.05, r_max=0.95, use_skip_connection=True, dropout=0)(x)
    x = lru.LRU_Block(N=1024, H=144, bidirectional=True, max_phase=2*pi/10, r_min=0.05, r_max=0.95, use_skip_connection=True, dropout=0)(x)
    x = lru.LRU_Block(N=1024, H=144, bidirectional=True, max_phase=2*pi/10, r_min=0.05, r_max=0.95, use_skip_connection=True, dropout=0)(x)
    x = lru.LRU_Block(N=1024, H=144, bidirectional=True, max_phase=2*pi/10, r_min=0.05, r_max=0.95, use_skip_connection=True, dropout=0)(x)
    x = lru.LRU_Block(N=1024, H=144, bidirectional=True, max_phase=2*pi/10, r_min=0.05, r_max=0.95, use_skip_connection=True, dropout=0)(x)
    x = lru.LRU_Block(N=1024, H=144, bidirectional=True, max_phase=2*pi/10, r_min=0.05, r_max=0.95, use_skip_connection=False, dropout=0)(x)
    x = lru.LRU_Block(N=1024, H=144, bidirectional=True, max_phase=2*pi/10, r_min=0.05, r_max=0.95, residual=False, use_skip_connection=False, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=1e-6, warmup_end=2e-4, warmup_steps=31250,
        initial_learning_rate=2e-4, decay_steps=2200000, alpha=0.05, t_mul=0.05, m_mul=0.05)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.02)
if self.use_nonlin:
    self.denseW = tf.keras.layers.Dense(H, activation='linear')
    self.denseV = tf.keras.layers.Dense(H, activation='tanh')
Params 9.083.000
Mean accuracy 89.0
Min loss 168.3
Result: Training stopped after 45 full epochs due to time limit. Loss had not converged yet. No overfitting yet. Lambdas moved but had by far not converged yet.

Test 62:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(512, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = lru.LRU_Block(N=512, H=160, bidirectional=True, max_phase=2*pi/1, r_min=0.10, r_max=0.90, residual=True, use_skip_connection=True, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=160, bidirectional=True, max_phase=2*pi/1, r_min=0.10, r_max=0.90, residual=True, use_skip_connection=True, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=160, bidirectional=True, max_phase=2*pi/1, r_min=0.10, r_max=0.90, residual=True, use_skip_connection=True, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=160, bidirectional=True, max_phase=2*pi/1, r_min=0.10, r_max=0.90, residual=True, use_skip_connection=True, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=160, bidirectional=True, max_phase=2*pi/1, r_min=0.10, r_max=0.90, residual=True, use_skip_connection=True, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=160, bidirectional=True, max_phase=2*pi/1, r_min=0.10, r_max=0.90, residual=True, use_skip_connection=True, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=160, bidirectional=True, max_phase=2*pi/1, r_min=0.10, r_max=0.90, residual=True, use_skip_connection=True, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=160, bidirectional=True, max_phase=2*pi/1, r_min=0.10, r_max=0.90, residual=True, use_skip_connection=False, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=160, bidirectional=True, max_phase=2*pi/1, r_min=0.10, r_max=0.90, residual=False, use_skip_connection=False, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=160, bidirectional=True, max_phase=2*pi/1, r_min=0.10, r_max=0.90, residual=False, use_skip_connection=False, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=160, bidirectional=True, max_phase=2*pi/1, r_min=0.10, r_max=0.90, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=160, bidirectional=True, max_phase=2*pi/1, r_min=0.10, r_max=0.90, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=8e-5, warmup_end=2e-4, warmup_steps=31250,
        initial_learning_rate=2e-4, decay_steps=2500000, alpha=0.0025, t_mul=0.06, m_mul=0.025)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.02)
Params 7.667.000
Mean accuracy 92.4
Min loss 117.8
Result: Training stopped after 51 full epochs due to time limit. Loss did not converge yet. Slight overfitting. Lambdas by far did not converge yet.

Test 63:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(1024, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = lru.LRU_Block(N=512, H=192, bidirectional=True, max_phase=2*pi/1, r_min=0.02, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=192, bidirectional=True, max_phase=2*pi/1, r_min=0.02, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=192, bidirectional=True, max_phase=2*pi/1, r_min=0.02, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=192, bidirectional=True, max_phase=2*pi/1, r_min=0.02, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=192, bidirectional=True, max_phase=2*pi/1, r_min=0.02, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=192, bidirectional=True, max_phase=2*pi/1, r_min=0.02, r_max=0.98, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=5e-5, warmup_end=2e-4, warmup_steps=31250,
        initial_learning_rate=2e-4, decay_steps=31250, alpha=0.005, t_mul=1, m_mul=1)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.005)
def custom_sigmoid(x):
    return(2*tf.sigmoid(x)-1)
if self.use_nonlin:
    self.denseW = tf.keras.layers.Dense(H, activation='linear')
    self.denseV = tf.keras.layers.Dense(H, activation=custom_sigmoid)
Params 6.026.000
Mean accuracy 91.2
Min loss 136.8
Result: Training stopped after 86 full epochs due to time limit. Loss did not converge yet. No overfitting. Lambdas by far did not converge yet.

Test 64:
    tf.keras.mixed_precision.set_global_policy('mixed_float16')
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(inputs)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(16, kernel_size=5, activation='swish', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    x = tf.keras.layers.Conv1D(1024, kernel_size=19, strides=6, activation='tanh', padding='same', use_bias=True)(x)
    x = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-5)(x)
    tf.keras.mixed_precision.set_global_policy('float32')
    x = lru.LRU_Block(N=512, H=192, bidirectional=True, max_phase=2*pi/1, r_min=0.02, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=192, bidirectional=True, max_phase=2*pi/1, r_min=0.02, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=192, bidirectional=True, max_phase=2*pi/1, r_min=0.02, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=192, bidirectional=True, max_phase=2*pi/1, r_min=0.02, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=192, bidirectional=True, max_phase=2*pi/1, r_min=0.02, r_max=0.98, dropout=0)(x)
    x = lru.LRU_Block(N=512, H=192, bidirectional=True, max_phase=2*pi/1, r_min=0.02, r_max=0.98, dropout=0)(x)
    classes = tf.keras.layers.Dense(NUM_CLASSES, use_bias=True, dtype=tf.float32)(x)
    classes = layers.ClipLayer(-5, 5)(classes)
    scheduler = schedulers.WarmUpCosineDecayWithRestarts(
        warmup_initial=5e-5, warmup_end=2e-4, warmup_steps=31250,
        initial_learning_rate=2e-4, decay_steps=31250, alpha=0.005, t_mul=1, m_mul=1)
    optimizer = tf.keras.optimizers.AdamW(learning_rate=scheduler, weight_decay=0.005)
def custom_sigmoid(x):
    return (tf.sigmoid(x)-0.5)*2
if self.use_nonlin:
    self.denseW = tf.keras.layers.Dense(H, activation='swish')
    self.denseV = tf.keras.layers.Dense(H, activation=custom_sigmoid)
Params 6.026.000
Mean accuracy 91.8
Min loss 127.6
Result: Training stopped after 85 full epochs due to time limit. Loss did not converge yet. No overfitting. Lambdas by far did not converge yet.
